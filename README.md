# Domain-Specific-Chatbot-Development-Using-Fine-Tuned-LLMs
• Fine-tuned the Mistral-7B model to create a course-specific chatbot for answering academic queries.

• Implemented Low-Rank Adaptation (LoRA) to enable efficient fine-tuning on limited computational resources and optimized the model using 4-bit precision for enhanced memory efficiency.

• Designed and curated a custom dataset of 100 question-answer pairs based on the course syllabus, achieving improved performance metrics, such as BLEU and ROUGE scores, through fine-tuning.

• Configured training parameters, such as batch size, epochs, learning rate, and employed cross-entropy loss for model evaluation, selecting the best model based on validation loss.

• Collaborated with a team of 3 members.
